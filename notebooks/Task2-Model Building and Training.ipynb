{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Model Building and Training\n",
    "2.1 Data Preparation\n",
    "Separate features and target variables.\n",
    "\n",
    "Feature Engineering &\n",
    "Perform train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static features shape: (120889, 5)\n",
      "Sequential features shape: 120889 sequences\n",
      "Target variable shape: (120889,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/cqk4jd793hzd58k1ssq6mkd40000gn/T/ipykernel_17729/1700115874.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sequential_data = fraud_data.groupby('user_id').apply(lambda x: x[['purchase_value', 'hour_of_day']].values.tolist())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "fraud_data = pd.read_csv('../data/fraud_data.csv')\n",
    "\n",
    "# Ensure 'purchase_time' is in datetime format\n",
    "fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time'])\n",
    "\n",
    "# Create 'hour_of_day' feature\n",
    "fraud_data['hour_of_day'] = fraud_data['purchase_time'].dt.hour\n",
    "\n",
    "# Sort data by user_id and purchase_time\n",
    "fraud_data = fraud_data.sort_values(by=['user_id', 'purchase_time'])\n",
    "\n",
    "# Static features for XGBoost\n",
    "static_features = ['purchase_value', 'age', 'device_id', 'browser', 'source']\n",
    "X_static = fraud_data[static_features]\n",
    "\n",
    "# Group by user and create sequences for LSTM\n",
    "sequential_data = fraud_data.groupby('user_id').apply(lambda x: x[['purchase_value', 'hour_of_day']].values.tolist())\n",
    "X_sequential = sequential_data.tolist()\n",
    "\n",
    "# Target variable\n",
    "y = fraud_data['class'].values\n",
    "\n",
    "# Train-test split\n",
    "X_static_train, X_static_test, X_sequential_train, X_sequential_test, y_train, y_test = train_test_split(\n",
    "    X_static, X_sequential, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Static features shape: {X_static_train.shape}\")\n",
    "print(f\"Sequential features shape: {len(X_sequential_train)} sequences\")\n",
    "print(f\"Target variable shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2Train XGBoost on Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static features shape: (12088, 14778)\n",
      "Target variable shape: (12088,)\n",
      "XGBoost AUC-ROC: 0.5935361087599402\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load data\n",
    "fraud_data = pd.read_csv('../data/fraud_data.csv')\n",
    "\n",
    "# Reduce dataset size (use 10% of the data)\n",
    "fraud_data = fraud_data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Ensure 'purchase_time' is in datetime format\n",
    "fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time'])\n",
    "\n",
    "# Create 'hour_of_day' feature\n",
    "fraud_data['hour_of_day'] = fraud_data['purchase_time'].dt.hour\n",
    "\n",
    "# Sort data by user_id and purchase_time\n",
    "fraud_data = fraud_data.sort_values(by=['user_id', 'purchase_time'])\n",
    "\n",
    "# Static features for XGBoost\n",
    "static_features = ['purchase_value', 'age', 'device_id', 'browser', 'source']\n",
    "X_static = fraud_data[static_features]\n",
    "\n",
    "# Encode categorical columns using one-hot encoding\n",
    "categorical_cols = ['device_id', 'browser', 'source']\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_cols = encoder.fit_transform(X_static[categorical_cols])\n",
    "\n",
    "# Combine encoded categorical columns with numerical columns\n",
    "numerical_cols = ['purchase_value', 'age']\n",
    "X_static_encoded = np.hstack([X_static[numerical_cols].values, encoded_cols])\n",
    "\n",
    "# Target variable\n",
    "y = fraud_data['class'].values\n",
    "\n",
    "# Train-test split\n",
    "X_static_train, X_static_test, y_train, y_test = train_test_split(\n",
    "    X_static_encoded, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Static features shape: {X_static_train.shape}\")\n",
    "print(f\"Target variable shape: {y_train.shape}\")\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_static_train, y_train)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "y_pred_xgb = xgb_model.predict_proba(X_static_test)[:, 1]\n",
    "print(f\"XGBoost AUC-ROC: {roc_auc_score(y_test, y_pred_xgb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM on Sequential Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, LSTM, Dense, Masking\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Convert input data to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Calculate max_length for padding\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_length, padding='post', dtype='float32')\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_length, padding='post', dtype='float32')\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(max_length, X_train.shape[-1]))\n",
    "masking_layer = Masking(mask_value=0.0)(input_layer)\n",
    "lstm_layer = LSTM(64, return_sequences=False)(masking_layer)\n",
    "dense_1 = Dense(32, activation='relu')(lstm_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_1)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_padded, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.2,\n",
    "                   verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_padded)\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "print(f'AUC-ROC Score: {auc_roc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Outputs Using a Meta-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Combine predictions from XGBoost and LSTM\n",
    "X_meta_train = np.column_stack((y_pred_xgb, y_pred_lstm.flatten()))\n",
    "\n",
    "# Train meta-model (logistic regression)\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(X_meta_train, y_test)\n",
    "\n",
    "# Evaluate hybrid model\n",
    "y_pred_hybrid = meta_model.predict_proba(X_meta_train)[:, 1]\n",
    "print(f\"Hybrid Model AUC-ROC: {roc_auc_score(y_test, y_pred_hybrid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation\n",
    "Advanced Evaluation Metrics\n",
    "We'll evaluate the model using:\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "Precision, Recall, F1-Score\n",
    "\n",
    "AUC-ROC\n",
    "\n",
    "Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, (y_pred_xgb > 0.5).astype(int))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, (y_pred_xgb > 0.5).astype(int)))\n",
    "\n",
    "# AUC-ROC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_xgb)\n",
    "print(f\"AUC-ROC: {roc_auc}\")\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_xgb)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"Precision-Recall AUC: {pr_auc}\")\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'Precision-Recall Curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MLOps Implementation\n",
    "Experiment Tracking with MLflow\n",
    "We'll use MLflow to track experiments, log metrics, and save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"XGBoost\")\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"AUC-ROC\", roc_auc)\n",
    "    mlflow.log_metric(\"Precision-Recall AUC\", pr_auc)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_model, \"xgboost_model\")\n",
    "    \n",
    "    # Log artifacts (e.g., plots)\n",
    "    plt.savefig(\"precision_recall_curve.png\")\n",
    "    mlflow.log_artifact(\"precision_recall_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
