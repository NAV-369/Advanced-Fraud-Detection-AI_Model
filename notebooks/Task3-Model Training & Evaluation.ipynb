{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Analysis Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved preprocessed data to: preprocessed_data.pkl\n",
      "\n",
      "Data shapes:\n",
      "X_train: (120889, 15)\n",
      "X_test: (30223, 15)\n",
      "y_train: (120889,)\n",
      "y_test: (30223,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Load the original data\n",
    "fraud_data = pd.read_csv('../data/fraud_data.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# 1. Convert timestamps\n",
    "fraud_data['signup_time'] = pd.to_datetime(fraud_data['signup_time']).astype(np.int64) // 10**9\n",
    "fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time']).astype(np.int64) // 10**9\n",
    "\n",
    "# 2. Convert categorical variables\n",
    "categorical_features = ['source', 'browser', 'sex']\n",
    "fraud_data = pd.get_dummies(fraud_data, columns=categorical_features)\n",
    "\n",
    "# 3. Handle ip_address\n",
    "fraud_data['ip_address'] = pd.util.hash_array(fraud_data['ip_address'].values)\n",
    "\n",
    "# 4. Create feature matrix X and target vector y\n",
    "X = fraud_data.drop(['class', 'user_id', 'device_id'], axis=1)\n",
    "y = fraud_data['class']\n",
    "\n",
    "# 5. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 6. Save the preprocessed data\n",
    "joblib.dump((X_train, X_test, y_train, y_test), 'preprocessed_data.pkl')\n",
    "print(\"Successfully saved preprocessed data to: preprocessed_data.pkl\")\n",
    "\n",
    "# Optional: Print shapes to verify\n",
    "print(\"\\nData shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Global Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the loaded model instead of 'model'\n",
    "explainer = shap.TreeExplainer(loaded_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
    "plt.title(\"Global Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_global.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved enhanced preprocessed data\n",
      "\n",
      "New Time-Based Features Added:\n",
      "1. time_since_signup: Hours between signup and purchase\n",
      "2. purchase_hour: Hour of the day (0-23)\n",
      "3. purchase_day: Day of week (0-6, 0=Monday)\n",
      "4. is_weekend: Binary indicator for weekend\n",
      "5. is_night: Binary indicator for night time purchases\n",
      "6. purchase_speed: Categorized speed of purchase after signup\n",
      "\n",
      "Data shapes:\n",
      "X_train: (120889, 22)\n",
      "X_test: (30223, 22)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the original data\n",
    "fraud_data = pd.read_csv('../data/fraud_data.csv')\n",
    "\n",
    "# Convert timestamp columns to datetime\n",
    "fraud_data['signup_time'] = pd.to_datetime(fraud_data['signup_time'])\n",
    "fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time'])\n",
    "\n",
    "# 1. Time difference between signup and purchase (in hours)\n",
    "fraud_data['time_since_signup'] = (fraud_data['purchase_time'] - fraud_data['signup_time']).dt.total_seconds() / 3600\n",
    "\n",
    "# 2. Hour of purchase (to capture daily patterns)\n",
    "fraud_data['purchase_hour'] = fraud_data['purchase_time'].dt.hour\n",
    "\n",
    "# 3. Day of week (to capture weekly patterns)\n",
    "fraud_data['purchase_day'] = fraud_data['purchase_time'].dt.dayofweek\n",
    "\n",
    "# 4. Is weekend\n",
    "fraud_data['is_weekend'] = fraud_data['purchase_day'].isin([5, 6]).astype(int)\n",
    "\n",
    "# 5. Is night time (e.g., between 11PM and 5AM)\n",
    "fraud_data['is_night'] = fraud_data['purchase_hour'].isin(range(23, 24)).astype(int) | \\\n",
    "                        fraud_data['purchase_hour'].isin(range(0, 6)).astype(int)\n",
    "\n",
    "# 6. Purchase speed category (how quickly after signup)\n",
    "fraud_data['purchase_speed'] = pd.cut(\n",
    "    fraud_data['time_since_signup'],\n",
    "    bins=[-np.inf, 1, 24, 168, np.inf],  # 1 hour, 24 hours, 1 week, more\n",
    "    labels=['very_fast', 'fast', 'normal', 'slow']\n",
    ")\n",
    "\n",
    "# 7. Convert purchase speed to dummy variables\n",
    "purchase_speed_dummies = pd.get_dummies(fraud_data['purchase_speed'], prefix='purchase_speed')\n",
    "fraud_data = pd.concat([fraud_data, purchase_speed_dummies], axis=1)\n",
    "\n",
    "# Now proceed with the rest of your preprocessing\n",
    "# Convert categorical variables\n",
    "categorical_features = ['source', 'browser', 'sex']\n",
    "fraud_data = pd.get_dummies(fraud_data, columns=categorical_features)\n",
    "\n",
    "# Handle ip_address\n",
    "fraud_data['ip_address'] = pd.util.hash_array(fraud_data['ip_address'].values)\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "# Include new time-based features but exclude original datetime columns and purchase_speed\n",
    "features_to_drop = ['class', 'user_id', 'device_id', 'signup_time', 'purchase_time', 'purchase_speed']\n",
    "X = fraud_data.drop(features_to_drop, axis=1)\n",
    "y = fraud_data['class']\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Save preprocessed data with new features\n",
    "joblib.dump((X_train, X_test, y_train, y_test), 'preprocessed_data_with_time_features.pkl')\n",
    "print(\"Successfully saved enhanced preprocessed data\")\n",
    "\n",
    "# Print new features summary\n",
    "print(\"\\nNew Time-Based Features Added:\")\n",
    "print(\"1. time_since_signup: Hours between signup and purchase\")\n",
    "print(\"2. purchase_hour: Hour of the day (0-23)\")\n",
    "print(\"3. purchase_day: Day of week (0-6, 0=Monday)\")\n",
    "print(\"4. is_weekend: Binary indicator for weekend\")\n",
    "print(\"5. is_night: Binary indicator for night time purchases\")\n",
    "print(\"6. purchase_speed: Categorized speed of purchase after signup\")\n",
    "\n",
    "print(\"\\nData shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain your LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11321, number of negative: 109568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 742\n",
      "[LightGBM] [Info] Number of data points in the train set: 120889, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093648 -> initscore=-2.269886\n",
      "[LightGBM] [Info] Start training from score -2.269886\n",
      "\n",
      "Model Performance with Time Features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     27393\n",
      "           1       1.00      0.53      0.69      2830\n",
      "\n",
      "    accuracy                           0.96     30223\n",
      "   macro avg       0.98      0.76      0.83     30223\n",
      "weighted avg       0.96      0.96      0.95     30223\n",
      "\n",
      "AUC-ROC Score: 0.7694\n"
     ]
    }
   ],
   "source": [
    "# Load the enhanced data\n",
    "X_train, X_test, y_train, y_test = joblib.load('preprocessed_data_with_time_features.pkl')\n",
    "\n",
    "# Create and train the model\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=32,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the new model\n",
    "joblib.dump(lgb_model, 'best_model_LightGBM_with_time_features.joblib')\n",
    "\n",
    "# Check performance\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "y_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nModel Performance with Time Features:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SHAP Local Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11321, number of negative: 109568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 444\n",
      "[LightGBM] [Info] Number of data points in the train set: 120889, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093648 -> initscore=-2.269886\n",
      "[LightGBM] [Info] Start training from score -2.269886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud probability: 0.0392\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 1. Define current features\n",
    "current_features = [\n",
    "    'purchase_value', 'age', 'ip_address',\n",
    "    'source_Ads', 'source_Direct', 'source_SEO',\n",
    "    'browser_Chrome', 'browser_FireFox', 'browser_IE',\n",
    "    'browser_Opera', 'browser_Safari',\n",
    "    'sex_F', 'sex_M'\n",
    "]\n",
    "\n",
    "# 2. Train new model\n",
    "new_model = LGBMClassifier(n_estimators=1000, random_state=42)\n",
    "new_model.fit(X_train[current_features], y_train)\n",
    "\n",
    "# 3. Get a fraud case\n",
    "fraud_mask = y_test == 1\n",
    "fraud_case = X_test[fraud_mask].iloc[[0]][current_features]\n",
    "\n",
    "# 4. Create SHAP explanation\n",
    "explainer = shap.TreeExplainer(new_model)\n",
    "shap_values = explainer.shap_values(fraud_case)\n",
    "\n",
    "# 5. Create waterfall plot - using shap_values[1] for the fraud class\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.plots.waterfall(\n",
    "    shap.Explanation(\n",
    "        values=shap_values[1][0],  # Select fraud class (class 1) values\n",
    "        base_values=explainer.expected_value[1],  # Base value for fraud class\n",
    "        data=fraud_case.iloc[0],\n",
    "        feature_names=current_features\n",
    "    ),\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"Individual Fraud Case Explanation\")\n",
    "plt.savefig('shap_local.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 6. Print prediction\n",
    "print(f\"Fraud probability: {new_model.predict_proba(fraud_case)[:, 1][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LIME Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME explainer setup completed successfully\n",
      "\n",
      "Features used:\n",
      "Numerical: ['purchase_value', 'age', 'ip_address']\n",
      "Categorical: ['source_Ads', 'source_Direct', 'source_SEO', 'browser_Chrome', 'browser_FireFox', 'browser_IE', 'browser_Opera', 'browser_Safari', 'sex_F', 'sex_M']\n"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['purchase_value', 'age', 'ip_address']\n",
    "categorical_features = [\n",
    "    'source_Ads', 'source_Direct', 'source_SEO',\n",
    "    'browser_Chrome', 'browser_FireFox', 'browser_IE', 'browser_Opera', 'browser_Safari',\n",
    "    'sex_F', 'sex_M'\n",
    "]\n",
    "\n",
    "# Create LIME explainer\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train[numerical_features + categorical_features].values,\n",
    "    feature_names=numerical_features + categorical_features,\n",
    "    class_names=['Legit', 'Fraud'],\n",
    "    categorical_features=list(range(len(numerical_features), len(numerical_features + categorical_features))),\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "print(\"LIME explainer setup completed successfully\")\n",
    "print(\"\\nFeatures used:\")\n",
    "print(\"Numerical:\", numerical_features)\n",
    "print(\"Categorical:\", categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load('best_model_LightGBM_with_time_features.joblib')\n",
    "\n",
    "# Let's print the feature names the model was trained on\n",
    "print(\"Model's expected features:\")\n",
    "print(model.feature_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 Complete: SHAP explainer saved successfully\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import shap\n",
    "\n",
    "# First let's load the preprocessed data that was already saved\n",
    "X_train, X_test, y_train, y_test = joblib.load('preprocessed_data_with_time_features.pkl')\n",
    "\n",
    "# Load the model and create SHAP explainer\n",
    "model = joblib.load('best_model_LightGBM_with_time_features.joblib')\n",
    "shap_explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Save only the SHAP explainer\n",
    "joblib.dump(shap_explainer, 'shap_explainer.pkl')\n",
    "\n",
    "print(\"Task 3 Complete: SHAP explainer saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
